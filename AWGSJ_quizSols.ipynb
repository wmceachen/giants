{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Run the following cell to obtain the data points $(x,y)$ in tabular format. Replace the NaN values with values that make intuitive sense given the other data. Feel to free use the data generating process for $y$, provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3 * x - 10 + np.random.normal(0, 2, 100)\n",
    "y[0] = 'Nan'\n",
    "y[24] = 'Nan'\n",
    "y[50] = 'Nan'\n",
    "y[78] = 'Nan'\n",
    "y[99] = 'Nan'\n",
    "dat = pd.DataFrame({'x':x, 'y':y}).sample(frac = 1)\n",
    "dat.reset_index(drop=True, inplace=True)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "for i in range(len(dat['y'])):\n",
    "    if dat['y'].isnull()[i]:\n",
    "        # a reasonable prediction using the x-value\n",
    "        dat['y'][i] = dat['x'][i] * 3 - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the SVD of the following matrix and print its singular values (in array or matrix form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "A = np.array([2, 1, -2, 1, 0, 0, 0, 1, 0]).reshape(3,3)\n",
    "u, s, v = np.linalg.svd(A)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Order the eigenvalues of the following matrix and find the eigenvector corresponding to the largest eigenvalue. Hint: you can use the $\\verb|np.argmax|$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "A = np.array([2, 1, -2, 1, 0, 0, 0, 1, 0]).reshape(3,3)\n",
    "l, v = np.linalg.eig(A)\n",
    "max_l = np.argmax(l)\n",
    "v[max_l]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Find the SVD of the following matrix and scale the last two singular values to $0.1s_i(i+1)$, where $s_i$ is the $i^{th}$ singular value. Print the original X and the new X created with the scaled singular values and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "X = np.array([3,4,5,10,16,2,-1,2,1]).reshape(3,3)\n",
    "u, s, v = np.linalg.svd(X)\n",
    "for i in range(1, X.shape[1]):\n",
    "    s[-i] = 0.1 * s[-i] * (3-i)\n",
    "s = np.diag(s)\n",
    "u @ s @ v, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) You have some data $x$ and $y$ provided below. Assume the true generating function is linear, i.e. $y = w_0 + w_1 x + \\epsilon$ where $\\epsilon$ is random noise. First, construct a feature matrix $X$ with a column for  **$1$**'s, $x$, and $x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3*x + 3 + np.random.normal(0, 1, 100)\n",
    "ones = np.ones(100)\n",
    "x_sq = x ** 2\n",
    "full_arr = np.append(ones, x)\n",
    "X = np.append(full_arr, x_sq).reshape(3, 100).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Run PCA on your feature matrix $X$ with $\\verb|n_components = 2|$. Report your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "pca_X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Now, run CCA on your feature matrix $X$. Report the difference between PCA and CCA for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "cca = CCA(n_components=2)\n",
    "cca.fit(X, y)\n",
    "cca_X = cca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) We have provided some data with various classes below. You are tasked with running regression on the classes, similar to the Baseball problem. First, create a featurized matrix that one-hot-encodes the classes. Only create columns for class B and C. Class A will act as the \"base case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3 * x - 10 + np.random.normal(0, 2, 100)\n",
    "classes = ['A', 'B', 'C']\n",
    "classes_dat = random.choices(classes, k=100)\n",
    "dat = pd.DataFrame({'x':x, 'y':y, 'Class':classes_dat}).sample(frac = 1)\n",
    "dat.reset_index(drop=True, inplace=True)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "temp_array = np.array(dat['Class'])\n",
    "for c in classes[-2:]:\n",
    "    indicator = np.zeros(len(dat['Class']))\n",
    "    for j in range(len(temp_array)):\n",
    "        if temp_array[j] == c:\n",
    "            indicator[j] = 1\n",
    "    #indicator = pd.Series(indicator, name = position)\n",
    "#     print(indicator)\n",
    "#     print(len(temp_array))\n",
    "    dat[c] = indicator\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) What is the main difference between CCA and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** PCA is an unsupervised strategy, while CCA is supervised. PCA will find the features corresponding to the largest eigenvalues, which give the direction of highest linear stretch, indicating these are the features with maximum variability. CCA tries to maximize the correlation between two datasets in order to create the best linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Explain why during one-hot-encoding, we keep one class as the \"base case.\" What would happen if we one-hot-encoded all the classes and tried to solve for the least squares solution? Hint: recall the least squares solution involves the term $(X^T X)^{-1}$. In order for $X^T X$ to be invertible, what must be true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Consider a basic example in which there are two classes, A and B, and only five data points. Suppose you one-hot-encode these two classes to obtain a vector $I_A = (1, 0, 1, 1, 1)$ and $I_B = (0, 1, 0, 0, 0)$. It is very clear that $I_B = 1 - I_A$ (by definition, you must be either class A or class B), making our feature matrix $X$ have linearly dependent columns. Thus, when we try to find the least squares solution, it is obvious that $X^T X$ will be non-invertible due to the linear dependency of X. You can extend this to any $k$ number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
