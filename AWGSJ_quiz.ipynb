{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Run the following cell to obtain the data points $(x,y)$ in tabular format. Replace the NaN values with values that make intuitive sense given the other data. Feel to free use the data generating process for $y$, provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3 * x - 10 + np.random.normal(0, 2, 100)\n",
    "y[0] = 'Nan'\n",
    "y[24] = 'Nan'\n",
    "y[50] = 'Nan'\n",
    "y[78] = 'Nan'\n",
    "y[99] = 'Nan'\n",
    "dat = pd.DataFrame({'x':x, 'y':y}).sample(frac = 1)\n",
    "dat.reset_index(drop=True, inplace=True)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the SVD of the following matrix and print its singular values (in array or matrix form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2, 1, -2, 1, 0, 0, 0, 1, 0]).reshape(3,3)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Order the eigenvalues of the following matrix and find the eigenvector corresponding to the largest eigenvalue. Hint: you can use the $\\verb|np.argmax|$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2, 1, -2, 1, 0, 0, 0, 1, 0]).reshape(3,3)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Find the SVD of the following matrix and scale the last two singular values to $0.1s_i(i+1)$, where $s_i$ is the $i^{th}$ singular value. Print the original X and the new X created with the scaled singular values and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([3,4,5,10,16,2,-1,2,1]).reshape(3,3)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) You have some data $x$ and $y$ provided below. Assume the true generating function is linear, i.e. $y = w_0 + w_1 x + \\epsilon$ where $\\epsilon$ is random noise. First, construct a feature matrix with a column for  **$1$**'s, $x$, and $x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3*x + 3 + np.random.normal(0, 1, 100)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Run PCA on your feature matrix $X$ with $\\verb|n_components = 2|$. Report your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Now, run CCA on your feature matrix $X$. Report the difference between PCA and CCA for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) We have provided some data with various classes below. You are tasked with running regression on the classes, similar to the Baseball problem. First, create a featurized matrix that one-hot-encodes the classes. Only create columns for class B and C. Class A will act as the \"base case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 3 * x - 10 + np.random.normal(0, 2, 100)\n",
    "classes = ['A', 'B', 'C']\n",
    "classes_dat = random.choices(classes, k=100)\n",
    "dat = pd.DataFrame({'x':x, 'y':y, 'Class':classes_dat}).sample(frac = 1)\n",
    "dat.reset_index(drop=True, inplace=True)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) What is the main difference between CCA and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Explain why during one-hot-encoding, we keep one class as the \"base case.\" What would happen if we one-hot-encoded all the classes and tried to solve for the least squares solution? Hint: recall the least squares solution involves the term $(X^T X)^{-1}$. In order for $X^T X$ to be invertible, what must be true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
